{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/malkiAbdelhamid/Advanced-Deep-Learning-2023-2024-esisba/master/lab1_QLearning/requirements_lab1.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we studied the Q-Learning algorithm, let’s implement it from scratch and train our Q-Learning agent in Taxi-3 environment:\n",
    "\n",
    "\n",
    "*Q-Learning* **is the RL algorithm that**:\n",
    "\n",
    "- Trains *Q-Function*, an **action-value function** that encoded, in internal memory, by a *Q-table* **that contains all the state-action pair values.**\n",
    "\n",
    "- Given a state and action, our Q-Function **will search the Q-table for the corresponding value.**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg\" alt=\"Q function\"  width=\"100%\"/>\n",
    "\n",
    "- When the training is done,**we have an optimal Q-Function, so an optimal Q-Table.**\n",
    "\n",
    "- And if we **have an optimal Q-function**, we\n",
    "have an optimal policy, since we **know for, each state, the best action to take.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import the packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import imageio"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### QAgent Class:\n",
    "Is initialized with the environment *env* and the necessary hyper-parameters (*alpha*, *gamma*, *epsilon*, etc)\n",
    "\n",
    "This class implements the most important steps of **Q-Learning** Algorithm:\n",
    "-  Q-Table initialization\n",
    "    -  Q-Table shape=(n_states * n_actions)\n",
    "\n",
    "-  Epsilon-greedy policy as an acting policy\n",
    "   - With *probability 1-ɛ* : **we do exploitation** (i.e. our agent selects the action with the highest state-action pair value).\n",
    "\n",
    "   - With *probability ɛ*: we do **exploration** (trying a random action).\n",
    "\n",
    "-  Greedy-policy as an updating policy\n",
    "   - using Bellman equation *Q(s,a) + lr[R(s,a) + gamma * max Q(s',a') - Q(s,a)]*\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f22799",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent:\n",
    "\n",
    "    def __init__(self, env, alpha, gamma, max_epsilon, min_epsilon, decay_rate):\n",
    "       \n",
    "        self.env = env\n",
    "\n",
    "        # table with q-values: n_states * n_actions\n",
    "        self.q_table=#--- add code here----------\n",
    "\n",
    "        # hyper-parameters\n",
    "        self.alpha = alpha                      # learning rate\n",
    "        self.gamma = gamma                      # discount factor\n",
    "        self.max_epsilon = max_epsilon          # Exploration probability at start\n",
    "        self.min_epsilon = min_epsilon          # Minimum exploration probability\n",
    "        self.decay_rate =  decay_rate           # Exponential decay rate for exploration prob\n",
    "    \n",
    "\n",
    "    \n",
    "    # get action using epsilon greedy policy\n",
    "    def get_action(self, state, epsilon):\n",
    "        # Randomly generate a number between 0 and 1\n",
    "        random_num =#--- add code here----------\n",
    "    \n",
    "        # if random_num > greater than epsilon --> exploitation\n",
    "        if random_num > epsilon:\n",
    "           # Take the action with the highest value given a state\n",
    "           action = #--- add code here----------\n",
    "    \n",
    "        # else --> exploration\n",
    "        else:\n",
    "           action = #--- add code here----------\n",
    "\n",
    "        return action\n",
    "\n",
    "    def update_parameters(self, state, action, reward, next_state):\n",
    "        \"\"\"\"\"\"\n",
    "        # Q-learning formula\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a)  + gamma * max Q(s',a') - Q(s,a)]\n",
    "        old_value = #--- add code here----------\n",
    "        next_max = #--- add code here----------\n",
    "        new_value = #--- add code here----------\n",
    "\n",
    "        # update the q_table\n",
    "        self.q_table[state, action] = new_value"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The training loop:\n",
    "The training is based on *Temporal Difference (TD) learning* where the Q-Table is updated after each step\n",
    "\n",
    "- For episode in the total of training episodes:\n",
    "\n",
    "  - Reduce epsilon (since we need less and less exploration)\n",
    "  - Reset the environment\n",
    "\n",
    "  - For step in max timesteps:\n",
    "    - Choose the action $a$ using epsilon greedy policy\n",
    "    - Take the action ($a$) and observe the outcome state(s') and reward (r)\n",
    "    - Update the Q-value Q($s$,$a$) using Bellman equation Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "    - If done, finish the episode\n",
    "    - Our next state is the new state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(n_training_episodes,  env, agent):\n",
    "\n",
    "  episode_rewards = []\n",
    "  episode_penalties = []\n",
    "  episode_steps = []\n",
    "  for episode in tqdm(range(n_training_episodes)):\n",
    "\n",
    "    total_rewards_ep = 0\n",
    "    total_penalties_ep=0\n",
    "    total_steps_ep=0\n",
    "\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = agent.min_epsilon + (agent.max_epsilon - agent.min_epsilon)*np.exp(-agent.decay_rate*episode)\n",
    "\n",
    "    # Reset the environment\n",
    "    state, info = env.reset()\n",
    "    step = 0\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    done=False\n",
    "\n",
    "    # repeat\n",
    "    while not done:\n",
    "      # Choose the action At using epsilon greedy policy\n",
    "      action = #--- add code here----------\n",
    "\n",
    "      # Take action At and observe Rt+1 and St+1\n",
    "      # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "      next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "      total_rewards_ep += reward\n",
    "      total_steps_ep +=1\n",
    "      if reward == -10:\n",
    "            total_penalties_ep += 1\n",
    "\n",
    "      # Update Q(s,a):= Q(s,a) + lr [R(s,a)  + gamma * max Q(s',a') - Q(s,a)]\n",
    "      #--- add code here----------\n",
    "\n",
    "      # If terminated or truncated finish the episode\n",
    "      done=terminated or truncated\n",
    "\n",
    "      if done:\n",
    "        break\n",
    "\n",
    "      # Our next state is the new state\n",
    "      #--- add code here----------\n",
    "\n",
    "    episode_rewards.append(total_rewards_ep)\n",
    "    episode_steps.append(total_steps_ep)\n",
    "    episode_penalties.append(total_penalties_ep)\n",
    "\n",
    "  return  episode_rewards,episode_steps, episode_penalties\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyper-Parameters\n",
    "\n",
    "The exploration related hyper-paramters are some of the most important ones.\n",
    "- We need to make sure that our agent **explores enough of the state space** to learn a good value approximation. To do that, we need to have progressive decay of the epsilon.\n",
    "- If you decrease epsilon too fast (too high decay_rate), **you take the risk that your agent will be stuck**, since your agent didn't explore enough of the state space and hence can't solve the problem."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_training_episodes = 1000  # Total training episodes\n",
    "alpha= 0.7                   # Learning rate\n",
    "\n",
    "# Environment parameters\n",
    "gamma = 0.95                 # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.05            # Minimum exploration probability\n",
    "decay_rate = 0.0005           # Exponential decay rate for exploration prob\n",
    "\n",
    "# Evaluation parameters\n",
    "n_eval_episodes = 100        # Total number of test episodes\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train the Q-Agent on Taxiv3 Environment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env=gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n",
    "\n",
    "agent=#--- add code here----------\n",
    "\n",
    "episode_rewards,episode_steps, episode_penalties=train(n_training_episodes, env,agent)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot the training result\n",
    "- episode_rewards: the cumulative rewards progression over all episodes (should be increased over time)\n",
    "- episode_steps: the required step per episode (should be decreased over time)\n",
    "- episode_penalties: the total penalties per episode (should be decreased over time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 4))\n",
    "ax.set_title(\"Cumulative Rewards per episode\")\n",
    "pd.Series(episode_rewards).plot(kind='line')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 4))\n",
    "ax.set_title(\"Timesteps to complete an eposide\")\n",
    "pd.Series(episode_steps).plot(kind='line')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 4))\n",
    "ax.set_title(\"Penalties per episode\")\n",
    "pd.Series(episode_penalties).plot(kind='line')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate the QAgent after training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_agent(env,agent,  n_eval_episodes):\n",
    "  \"\"\"\n",
    "  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
    "  :param agent: the gent within its  evaluation environment and Qtable\n",
    "  :param max_steps: Maximum number of steps per episode\n",
    "  :param n_eval_episodes: Number of episode to evaluate the agent\n",
    "   \"\"\"\n",
    "  episode_rewards = []\n",
    "  episode_penalties = []\n",
    "  episode_steps = []\n",
    "  for episode in tqdm(range(n_eval_episodes)):\n",
    "\n",
    "    state, info= env.reset()\n",
    "    step = 0\n",
    "    truncated = False\n",
    "    terminated = False\n",
    "    total_rewards_ep = 0\n",
    "    total_penalties_ep=0\n",
    "    total_steps_ep=0\n",
    "    done= False\n",
    "\n",
    "    while not done:\n",
    "      # Take the action (index) that have the maximum expected future reward given that state\n",
    "      # we use epsilon=0 for exploitation\n",
    "\n",
    "      action = agent.get_action(state,0)\n",
    "      next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "      total_rewards_ep += reward\n",
    "      total_steps_ep+=1\n",
    "\n",
    "      if reward == -10:\n",
    "            total_penalties_ep += 1\n",
    "\n",
    "      done=terminated or truncated\n",
    "\n",
    "      if done:\n",
    "        break\n",
    "\n",
    "      state = next_state\n",
    "\n",
    "    episode_rewards.append(total_rewards_ep)\n",
    "    episode_steps.append(total_steps_ep)\n",
    "    episode_penalties.append(total_penalties_ep)\n",
    "\n",
    "  mean_reward = np.mean(episode_rewards)\n",
    "  std_reward = np.std(episode_rewards)\n",
    "\n",
    "  return mean_reward, std_reward, episode_rewards,episode_steps, episode_penalties"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mean_reward, std_reward,episode_rewards,episode_steps, episode_penalties=evaluate_agent(env,agent, 1000)\n",
    "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 4))\n",
    "ax.set_title(\"Cumulative Rewards per ride\")\n",
    "pd.Series(episode_rewards).plot(kind='line')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 4))\n",
    "ax.set_title(\"Timesteps to complete ride\")\n",
    "pd.Series(episode_steps).plot(kind='line')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 4))\n",
    "ax.set_title(\"Penalties per ride\")\n",
    "pd.Series(episode_penalties).plot(kind='line')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Record a simulation as a video"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def record_video(env, agent, out_directory, fps=1):\n",
    "  \"\"\"\n",
    "  Generate a replay video of the agent\n",
    "  :param env\n",
    "  :param agent:  agent within its Qtable\n",
    "  :param out_directory\n",
    "  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n",
    "  \"\"\"\n",
    "  images = []\n",
    "  terminated = False\n",
    "  truncated = False\n",
    "  state, info = env.reset(seed=random.randint(0,500))\n",
    "  img = env.render()\n",
    "  images.append(img)\n",
    "  while not terminated or truncated:\n",
    "    # Take the action (index) that have the maximum expected future reward given that state\n",
    "    action = np.argmax(agent.q_table[state][:])\n",
    "    state, reward, terminated, truncated, info = env.step(action) # We directly put next_state = state for recording logic\n",
    "    img = env.render()\n",
    "    images.append(img)\n",
    "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from base64 import b64encode\n",
    "from IPython.display import HTML\n",
    "\n",
    "# generate the video\n",
    "video_path = \"./replay.mp4\"\n",
    "record_video(env, agent, video_path, 1)\n",
    "\n",
    "# Show video\n",
    "mp4 = open(video_path,'rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\"\"\"<video width=400 controls>      <source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % data_url)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
