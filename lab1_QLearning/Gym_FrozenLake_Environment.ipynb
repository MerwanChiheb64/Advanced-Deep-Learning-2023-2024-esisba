{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/malkiAbdelhamid/Advanced-Deep-Learning-2023-2024-esisba/master/lab1_QLearning/requirements_lab1.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### ðŸ‘‰Before you solve a Reinforcement Learning problem you need to define what are\n",
    "- the environment\n",
    "- the states\n",
    "- the actions\n",
    "- the rewards\n",
    "\n",
    "#### ðŸ‘‰We are using the `FrozenLake-v1` environment from OpenAI's gym: https://www.gymlibrary.dev/environments/toy_text/frozen_lake/\n",
    "\n",
    "#### ðŸ‘‰`FrozenLake-v1` is an easy environment because the action space is small, and the state space is large but finite.\n",
    "\n",
    "#### ðŸ‘‰Environments with a finite number of actions and states are called tabular"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import the Gymnasium Library"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.envs.toy_text.frozen_lake import  generate_random_map\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a FrozenLake-v1 environment with gym.make()\n",
    "- default map=4x4\n",
    "- In order to display the environment's current state you need to add the parameter==> render_mode=\"rgb_array\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "env=gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"rgb_array\")\n",
    "#env=gym.make(\"FrozenLake-v1\",desc=generate_random_map(size=8), map_name=\"8x8\", is_slippery=False, render_mode=\"rgb_array\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### We reset the environment to its initial state with state = env.reset()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "state=env.reset()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### State space\n",
    "-  The state is a value representing the agentâ€™s current position as current_row * nrows + current_col (where both the row and col start at 0).\n",
    "- For example, the goal position in the 8x8 map can be calculated as follows: 7 * 8 + 7 = 63. The number of possible observations is dependent on the size of the map. For example, the 8x8 map has 64 possible states."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"State Space {}\".format(env.observation_space.n))\n",
    "print(env.observation_space.sample())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Action space :\n",
    "##### The agent takes a 1-element vector for actions. The action space is (dir), where dir decides direction to move in which can be:\n",
    "\n",
    "- 0: LEFT\n",
    "\n",
    "- 1: DOWN\n",
    "\n",
    "- 2: RIGHT\n",
    "\n",
    "- 3: UP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Action Space {}\".format(env.action_space.n))\n",
    "print(env.action_space.sample())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### env.render(): display the environment's current state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image=env.render()\n",
    "plt.imshow(image)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### env.step(n_action)--> next state, reward, terminated,truncated, info\n",
    "#### Updates an environment with actions returning:\n",
    " - the next agent state,\n",
    " - the reward for taking that actions,\n",
    " - if the environment has terminated or truncated due to the latest action\n",
    " - and information from the environment about the step"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#apply the right action,\n",
    "next_state, reward, isTerminated, isTruncated, _=env.step(2)\n",
    "print(next_state, reward, isTerminated, isTruncated)\n",
    "plt.imshow(env.render())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### First, reset the environment, then define the trajectory which is a set of necessary actions required  to achieve the goal\n",
    "#### Finally, record the different steps through a video"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "env.reset()\n",
    "images=[]\n",
    "images.append(env.render())\n",
    "\n",
    "#right, right, down, down, down, right\n",
    "\n",
    "env.step(2)\n",
    "images.append(env.render())\n",
    "\n",
    "env.step(2)\n",
    "images.append(env.render())\n",
    "\n",
    "env.step(1)\n",
    "images.append(env.render())\n",
    "\n",
    "env.step(1)\n",
    "images.append(env.render())\n",
    "\n",
    "env.step(1)\n",
    "images.append(env.render())\n",
    "\n",
    "env.step(2)\n",
    "images.append(env.render())\n",
    "\n",
    "imageio.mimsave('./render.mp4', [np.array(img) for i, img in enumerate(images)], fps=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from base64 import b64encode\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Show video\n",
    "compressed_path = 'replay.mp4'\n",
    "mp4 = open(compressed_path,'rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "\n",
    "HTML(\"\"\"<video width=400 controls>      <source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % data_url)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## vectorized environment gym\n",
    "#### We create a vectorized environment (a method for stacking multiple independent environments into a single environment) of 16 environments, this way, weâ€™ll have more diverse experiences during the training.\n",
    "https://gymnasium.farama.org/api/vector/"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "envs=gym.vector.make('FrozenLake-v1',  num_envs=4)\n",
    "env2=gym.make('FrozenLake-v1')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "envs.reset(seed=42)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env.reset()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
