{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/malkiAbdelhamid/Advanced-Deep-Learning-2023-2024-esisba/master/lab1_QLearning/requirements_lab1.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 👉We are using the `Taxi-v3` environment from OpenAI's gym: https://gym.openai.com/envs/Taxi-v3/\n",
    "\n",
    "#### 👉`Taxi-v3` is an easy environment because the action space is small, and the state space is large but finite.\n",
    "\n",
    "#### 👉Environments with a finite number of actions and states are called tabular\n",
    "\n",
    "#### Actions\n",
    "There are 6 discrete deterministic actions:\n",
    "\n",
    "- 0: move south\n",
    "\n",
    "- 1: move north\n",
    "\n",
    "- 2: move east\n",
    "\n",
    "- 3: move west\n",
    "\n",
    "- 4: pickup passenger\n",
    "\n",
    "- 5: drop off passenger\n",
    "#### Rewards\n",
    "- -1 per step unless other reward is triggered.\n",
    "- +20 delivering passenger.\n",
    "- -10 executing “pickup” and “drop-off” actions illegally."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env=gym.make(\"Taxi-v3\",  render_mode=\"rgb_array\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4f4dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    \n",
    "    def __init__(self,env):\n",
    "        self.env=env\n",
    "        \n",
    "    def get_Random_Action(self):\n",
    "         # -----add the code------------"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### create a random Agent interacting with \"Taxi-v3\" environment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent= #--- add code here----------"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train the Random agent on multiples episodes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "n_episodes = 100\n",
    "\n",
    "# For plotting metrics\n",
    "timesteps_per_episode = []\n",
    "cumReward_per_episode = []\n",
    "penalties_per_episode = []\n",
    "\n",
    "for i in tqdm(range(0, n_episodes)):\n",
    "\n",
    "    # reset environment to a random state\n",
    "    #--- add code here----------\n",
    "\n",
    "    # initialize the metrics\n",
    "    steps, penalties, cum_reward, = 0, 0, 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        # select an action randomly\n",
    "        action=#--- add code here----------\n",
    "\n",
    "        #execute the selected action on the environment and return the variables:\n",
    "        #==>next_state, reward, terminated and truncated\n",
    "        #--- add code here----------\n",
    "\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "        else:\n",
    "            cum_reward+=reward\n",
    "\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "        done=terminated or truncated\n",
    "\n",
    "\n",
    "    timesteps_per_episode.append(steps)\n",
    "    cumReward_per_episode.append(cum_reward)\n",
    "    penalties_per_episode.append(penalties)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd47159",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 4))\n",
    "ax.set_title(\"Timesteps to complete ride\")    \n",
    "pd.Series(timesteps_per_episode).plot(kind=\"line\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 4))\n",
    "ax.set_title(\"Cum_Reward to complete ride\")    \n",
    "pd.Series( cumReward_per_episode).plot(kind=\"line\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 4))\n",
    "ax.set_title(\"Penalties per ride\")    \n",
    "pd.Series(penalties_per_episode).plot(kind=\"line\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
