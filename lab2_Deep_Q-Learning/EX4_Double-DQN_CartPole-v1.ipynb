{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Double Deep Q-Network (DDQN)\n",
    "---\n",
    "In this notebook, you will implement a Double DQN agent with OpenAI Gym's CartPole-v1 environment.\n",
    "\n",
    "### Import the Necessary Packages"
   ],
   "metadata": {
    "collapsed": false,
    "id": "2p2cCsCnfP22"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ],
   "metadata": {
    "id": "y7AKn9TqfP25"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define some hyperparameter"
   ],
   "metadata": {
    "collapsed": false,
    "id": "d7Zq_7HAfP27"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate\n",
    "UPDATE_EVERY = 4        # how often to update the network"
   ],
   "metadata": {
    "id": "XmdvUFEDfP27"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "id": "16_GML3nfP27"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define Neural Network Architecture.\n",
    "\n",
    "Since `CartPole-v1` environment is sort of simple envs, we don't need complicated architecture. We just need non-linear function approximator that maps from state to action."
   ],
   "metadata": {
    "collapsed": false,
    "id": "9J83ze82fP27"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, state_shape, action_space_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_shape (int): Dimension of each state\n",
    "            action_space_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_shape, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_space_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = self.fc1(state)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x=self.fc3(x)\n",
    "        return x"
   ],
   "metadata": {
    "id": "6YNclskqfP28"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define Replay Buffer\n",
    "### Experience Replay\n",
    "\n",
    "To perform *experience replay* the authors store the agent's experiences $e_t$ as represented by the tuple\n",
    "\n",
    "$$ e_t = (s_t, a_t, r_t, s_{t+1}) $$\n",
    "\n",
    "consisting of the observed state in period $t$, the reward received in period $t$, the action taken in period $t$, and the resulting state in period $t+1$. The dataset of agent experiences at period $t$ consists of the set of past experiences.\n",
    "\n",
    "$$ D_t = \\{e1, e2, ..., e_t \\} $$\n",
    "\n",
    "Depending on the task it may note be feasible for the agent to store the entire history of past experiences.\n",
    "\n",
    "During learning Q-learning updates are computed based on samples (or minibatches) of experience $(s,a,r,s')$, drawn uniformly at random from the pool of stored samples $D_t$.\n",
    "\n",
    "The following is my Python implmentation of these ideas.\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "OjcPyfQ7fP29"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ],
   "metadata": {
    "id": "hlIePvuHfP29"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Improving the DQN Agent using Double Q-Learning\n",
    "\n",
    "The key idea behind Double Q-learning is to reduce overestimations of Q-values by separating the selection of actions from the evaluation of those actions so that a different Q-network can be used in each step. When applying Double Q-learning to extend the DQN algorithm one can use the online Q-network, $Q(S, a; \\theta)$, to select the actions and then the target Q-network, $Q(S, a; \\theta^{-})$, to evaluate the selected actions.\n",
    "\n",
    "$$ Y_t^{DoubleDQN} = R_{t+1} + \\gamma Q\\big(S_{t+1}, \\underset{a}{\\mathrm{argmax}}\\ Q(S_{t+1}, a; \\theta_t), \\theta_t^{-}\\big) $$\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "5ESIilDXfP2-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DDQAgent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_shape, action_space_size, seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state_shape (int): dimension of each state\n",
    "            action_space_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_shape = state_shape\n",
    "        self.action_space_size = action_space_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_shape, action_space_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_shape, action_space_size, seed).to(device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "            action_values = self.qnetwork_local(state)\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_space_size))\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        # Obtain random minibatch of tuples from D\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        ## Compute and minimize the loss\n",
    "        \"\"\"Select the greedy action for the next_state given Local Q-network.\"\"\"\n",
    "        _, actions_for_next_states = #-----Add Code Here---------------------\n",
    "\n",
    "        \"\"\"Compute the next_Q-values by evaluating the actions given the next_states and the Target Q-network.\"\"\"\n",
    "        q_targets_next = #-----Add Code Here---------------------\n",
    "\n",
    "        ### Calculate target value from bellman equation\n",
    "        q_targets = rewards + gamma * q_targets_next * (1 - dones)\n",
    "\n",
    "        ### Calculate expected value from local network\n",
    "        q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        ### Loss calculation (we used Mean squared error)\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ],
   "metadata": {
    "id": "yy-v66AgfP2_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "id": "XQAQGS7ofP3A"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define the Deep QLearning Network Agent\n",
    "\n",
    "Before Training the Double DQN Agent, we  re-implement the DQN Agent  in order to compare them\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DQAgent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_shape, action_space_size, seed):\n",
    "\n",
    "        self.state_shape = state_shape\n",
    "        self.action_space_size = action_space_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_shape, action_space_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_shape, action_space_size, seed).to(device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "            action_values = self.qnetwork_local(state)\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_space_size))\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "\n",
    "        # Obtain random minibatch of tuples from D\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        ## Compute and minimize the loss\n",
    "        ### Extract next maximum estimated value from target network\n",
    "        q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "\n",
    "        ### Calculate target value from bellman equation\n",
    "        q_targets = rewards + gamma * q_targets_next * (1 - dones)\n",
    "\n",
    "        ### Calculate expected value from local network\n",
    "        q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        ### Loss calculation (we used Mean squared error)\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training Process\n",
    "\n",
    "The code for the training loop remains unchanged For both agents (Double-DQN and DQN).\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "tCyWci_YfP3A"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def dqn(agent, n_episodes=1700, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "\n",
    "    torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "    return scores\n"
   ],
   "metadata": {
    "id": "aaTZRN5TfP3A"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating the Gym environment CartPole-v1\n",
    "<img src=\"https://gymnasium.farama.org/_images/cart_pole.gif\" alt=\"LunarLander-v2\"  width=\"100%\"/>\n",
    "\n",
    "This environment is part of the Classic Control environments which contains general information about the environment.\n",
    "\n",
    "\n",
    "### Description\n",
    "This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in “Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem”. A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart.\n",
    "\n",
    "### Action Space\n",
    "The action is a ndarray with shape (1,) which can take values {0, 1} indicating the direction of the fixed force the cart is pushed with.\n",
    "\n",
    "- 0: Push cart to the left\n",
    "\n",
    "- 1: Push cart to the right\n",
    "\n",
    "Note: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it\n",
    "\n",
    "### Observation Space\n",
    "The observation is a ndarray with shape (4,) with the values corresponding to the following positions and velocities:\n",
    "\n",
    "- Cart Position\n",
    "- Cart Velocity\n",
    "- Pole Angle\n",
    "- Pole Angular Velocity\n",
    "\n",
    "### Rewards\n",
    "Since the goal is to keep the pole upright for as long as possible, a reward of +1 for every step taken, including the termination step, is allotted. The threshold for rewards is 500"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Lq93jPK0fP3B"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "print(env.reset())\n",
    "print('State shape: ', env.observation_space.shape[0])\n",
    "print('Number of actions: ', env.action_space.n)\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Amss4kNwfP3B",
    "outputId": "0da4b16e-a5ae-4a82-f2dc-8b4d5a0bdcd3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "id": "Sh5Gxq7DfP3B"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training the `DDQagent` using Double DQN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DDQagent = DDQAgent(state_shape=env.observation_space.shape[0], action_space_size=env.action_space.n, seed=0)\n",
    "scores_DDQAgent = dqn(DDQagent)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qUYrNolWfP3B",
    "outputId": "ee4e271c-7e3c-48de-ba7f-101c145db2c0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training the `DQagent` using DQN\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DQagent = DQAgent(state_shape=env.observation_space.shape[0], action_space_size=env.action_space.n, seed=0)\n",
    "scores_DQAgent = dqn(DQagent)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "id": "H_byovfYfP3C"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Comparing Double DQN and  DQN\n",
    "#### Plotting the time series of scores (scores_DDQAgent & scores_DQAgent )\n",
    "\n",
    "I can use [Pandas](https://pandas.pydata.org/) to quickly plot the time series of scores along with a 100 episode moving average. Note that training stops as soon as the rolling average crosses the target score."
   ],
   "metadata": {
    "collapsed": false,
    "id": "m77NKqKMfP3C"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores_DDQAgent= pd.Series(scores_DDQAgent, name=\"scores_DDQAgent\")\n",
    "scores_DDQAgent.describe()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TEck5yEZfP3C",
    "outputId": "ba51c957-93e3-466b-f80a-367ca9cc7b3c"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores_DQAgent= pd.Series(scores_DQAgent, name=\"scores_DQAgent\")\n",
    "scores_DQAgent.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(10, 6), sharex=True, sharey=True)\n",
    "_ = scores_DDQAgent.plot(ax=ax[0], label=\"Scores\")\n",
    "_ = (scores_DDQAgent.rolling(window=100)\n",
    "           .mean()\n",
    "           .rename(\"Rolling Average\")\n",
    "           .plot(ax=ax[0]))\n",
    "_=ax[0].legend()\n",
    "_=ax[0].set_ylabel(\"Double DQN Score\")\n",
    "\n",
    "_ = scores_DQAgent.plot(ax=ax[1], label=\"Scores\")\n",
    "_ = (scores_DQAgent.rolling(window=100)\n",
    "           .mean()\n",
    "           .rename(\"Rolling Average\")\n",
    "           .plot(ax=ax[1]))\n",
    "_=ax[1].legend()\n",
    "_ = ax[1].set_ylabel(\"DQN Score\")\n",
    "_ = ax[1].set_xlabel(\"Episode Number\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "a5tFWRVrfP3D",
    "outputId": "28cd22de-d1da-4fd5-fd3c-6705a920d8f8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "id": "1XkkeQ_HfP3E"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
