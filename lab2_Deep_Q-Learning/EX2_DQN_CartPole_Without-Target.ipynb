{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Deep Q-Network (DQN) Vs Deep Q-Network without Target\n",
    "---\n",
    "In this notebook, you will implement two versions of Deep Q-Learning  agent:\n",
    " - DQN with a fixed Target network\n",
    " - DQN without Target network\n",
    "\n",
    "The two agents will be trained with OpenAI Gym's CartPole_v1 environment.\n",
    "\n",
    "### Import the Necessary Packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define some hyperparameter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate\n",
    "UPDATE_EVERY = 4        # how often to update the network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define Neural Network Architecture.\n",
    "\n",
    "Since `CartPole_v1` environment is sort of simple envs, we don't need complicated architecture. We just need non-linear function approximator that maps from state to action."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, state_shape, action_space_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_shape (int): Dimension of each state\n",
    "            action_space_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_shape, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_space_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = self.fc1(state)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x=self.fc3(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define Replay Buffer\n",
    "### Experience Replay\n",
    "\n",
    "To perform *experience replay* the authors store the agent's experiences $e_t$ as represented by the tuple\n",
    "\n",
    "$$ e_t = (s_t, a_t, r_t, s_{t+1}) $$\n",
    "\n",
    "consisting of the observed state in period $t$, the reward received in period $t$, the action taken in period $t$, and the resulting state in period $t+1$. The dataset of agent experiences at period $t$ consists of the set of past experiences.\n",
    "\n",
    "$$ D_t = \\{e1, e2, ..., e_t \\} $$\n",
    "\n",
    "Depending on the task it may note be feasible for the agent to store the entire history of past experiences.\n",
    "\n",
    "During learning Q-learning updates are computed based on samples (or minibatches) of experience $(s,a,r,s')$, drawn uniformly at random from the pool of stored samples $D_t$.\n",
    "\n",
    "The following is my Python implmentation of these ideas.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define the Deep QLearning Agent\n",
    "\n",
    "The Deep $Q$-learning update at iteration $i$ uses the following loss function\n",
    "\n",
    "$$ \\mathcal{L_i}(\\theta_i) = \\mathbb{E}_{(s, a, r, s') \\sim U(D)} \\Bigg[\\bigg(r + \\gamma \\max_{a'} Q\\big(s', a'; \\theta_i^{-}\\big) - Q\\big(s, a; \\theta_i\\big)\\bigg)^2\\Bigg] $$\n",
    "\n",
    "where $\\gamma$ is the discount factor determining the agent’s horizon, $\\theta_i$ are the parameters of the $Q$-network at iteration $i$ and $\\theta_i^{-}$ are the $Q$-network parameters used to compute the target at iteration $i$. The target network parameters $\\theta_i^{-}$ are only updated with the $Q$-network parameters $\\theta_i$ every $C$ steps and are held fixed between individual updates.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DQAgent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_shape, action_space_size, seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state_shape (int): dimension of each state\n",
    "            action_space_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_shape = state_shape\n",
    "        self.action_space_size = action_space_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_shape, action_space_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_shape, action_space_size, seed).to(device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "            action_values = self.qnetwork_local(state)\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_space_size))\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        # Obtain random minibatch of tuples from D\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        ## Compute and minimize the loss\n",
    "        ### Extract next maximum estimated value from target network\n",
    "        q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "\n",
    "        ### Calculate target value from bellman equation\n",
    "        q_targets = rewards + gamma * q_targets_next * (1 - dones)\n",
    "\n",
    "        ### Calculate expected value from local network\n",
    "        q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        ### Loss calculation (we used Mean squared error)\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define the Deep QLearning Agent Without Target Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DQAgent_Without_Target():\n",
    "\n",
    "    #----Todo------\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training Process\n",
    "\n",
    "The code for the training loop remains unchanged For both agents (DQN and DQN Without Target).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def dqn(agent, n_episodes=2500, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "\n",
    "    torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "    return scores\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating the Gym environment CartPole-v1\n",
    "<img src=\"https://gymnasium.farama.org/_images/cart_pole.gif\" alt=\"LunarLander-v2\"  width=\"100%\"/>\n",
    "\n",
    "This environment is part of the Classic Control environments which contains general information about the environment.\n",
    "\n",
    "\n",
    "### Description\n",
    "This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in “Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem”. A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart.\n",
    "\n",
    "### Action Space\n",
    "The action is a ndarray with shape (1,) which can take values {0, 1} indicating the direction of the fixed force the cart is pushed with.\n",
    "\n",
    "- 0: Push cart to the left\n",
    "\n",
    "- 1: Push cart to the right\n",
    "\n",
    "Note: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it\n",
    "\n",
    "### Observation Space\n",
    "The observation is a ndarray with shape (4,) with the values corresponding to the following positions and velocities:\n",
    "\n",
    "- Cart Position\n",
    "- Cart Velocity\n",
    "- Pole Angle\n",
    "- Pole Angular Velocity\n",
    "\n",
    "### Rewards\n",
    "Since the goal is to keep the pole upright for as long as possible, a reward of +1 for every step taken, including the termination step, is allotted. The threshold for rewards is 500"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "print(env.reset())\n",
    "print('State shape: ', env.observation_space.shape[0])\n",
    "print('Number of actions: ', env.action_space.n)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training the `DQagent` using  DQN\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.7993, 1.8824]])\n",
      "[[1.7992842 1.8823843]]\n",
      "1\n",
      "tensor([[1.5484, 1.6271]])\n",
      "tensor([[1.5484, 1.6271]])\n",
      "[[1.5484455 1.6271293]]\n",
      "1\n",
      "Episode 281\tAverage Score: 10.80tensor([[1.6812, 1.6993]])\n",
      "tensor([[1.6812, 1.6993]])\n",
      "[[1.6811563 1.6992593]]\n",
      "1\n",
      "tensor([[1.6906, 1.7391]])\n",
      "tensor([[1.6906, 1.7391]])\n",
      "[[1.6906244 1.7391055]]\n",
      "1\n",
      "tensor([[1.7235, 1.7819]])\n",
      "tensor([[1.7235, 1.7819]])\n",
      "[[1.7234689 1.7819084]]\n",
      "1\n",
      "tensor([[1.7397, 1.7996]])\n",
      "tensor([[1.7397, 1.7996]])\n",
      "[[1.7397374 1.7995505]]\n",
      "1\n",
      "tensor([[1.7754, 1.8430]])\n",
      "tensor([[1.7754, 1.8430]])\n",
      "[[1.7754024 1.8430353]]\n",
      "1\n",
      "tensor([[1.7124, 1.7907]])\n",
      "tensor([[1.7124, 1.7907]])\n",
      "[[1.7124246 1.7907474]]\n",
      "1\n",
      "tensor([[1.4482, 1.5036]])\n",
      "tensor([[1.4482, 1.5036]])\n",
      "[[1.4481932 1.5036294]]\n",
      "1\n",
      "Episode 282\tAverage Score: 10.81tensor([[1.6964, 1.7220]])\n",
      "tensor([[1.6964, 1.7220]])\n",
      "[[1.6963649 1.7220179]]\n",
      "1\n",
      "tensor([[1.7093, 1.7614]])\n",
      "tensor([[1.7093, 1.7614]])\n",
      "[[1.7093134 1.7613913]]\n",
      "1\n",
      "tensor([[1.7436, 1.8023]])\n",
      "tensor([[1.7436, 1.8023]])\n",
      "[[1.7436222 1.8022574]]\n",
      "1\n",
      "tensor([[1.7590, 1.8188]])\n",
      "tensor([[1.7590, 1.8188]])\n",
      "[[1.7589717 1.8187917]]\n",
      "1\n",
      "tensor([[1.7439, 1.8058]])\n",
      "tensor([[1.7439, 1.8058]])\n",
      "[[1.7438589 1.8058074]]\n",
      "1\n",
      "tensor([[1.7457, 1.8178]])\n",
      "tensor([[1.7457, 1.8178]])\n",
      "[[1.7456728 1.8178493]]\n",
      "1\n",
      "tensor([[1.4022, 1.4748]])\n",
      "tensor([[1.4022, 1.4748]])\n",
      "[[1.4021869 1.4747845]]\n",
      "1\n",
      "Episode 283\tAverage Score: 10.80tensor([[1.6986, 1.7005]])\n",
      "tensor([[1.6986, 1.7005]])\n",
      "[[1.6986169 1.7004625]]\n",
      "1\n",
      "tensor([[1.7028, 1.7072]])\n",
      "tensor([[1.7028, 1.7072]])\n",
      "[[1.70282   1.7072272]]\n",
      "1\n",
      "tensor([[1.6982, 1.7264]])\n",
      "tensor([[1.6982, 1.7264]])\n",
      "[[1.6981921 1.7264167]]\n",
      "1\n",
      "tensor([[1.7138, 1.7710]])\n",
      "tensor([[1.7138, 1.7710]])\n",
      "[[1.7137513 1.7709912]]\n",
      "1\n",
      "tensor([[1.7411, 1.8116]])\n",
      "tensor([[1.7411, 1.8116]])\n",
      "[[1.7411137 1.8115816]]\n",
      "1\n",
      "tensor([[1.7915, 1.8691]])\n",
      "tensor([[1.7915, 1.8691]])\n",
      "[[1.7914541 1.8690705]]\n",
      "1\n",
      "tensor([[1.7774, 1.8512]])\n",
      "tensor([[1.7774, 1.8512]])\n",
      "[[1.7773631 1.8512244]]\n",
      "1\n",
      "tensor([[1.8160, 1.8979]])\n",
      "tensor([[1.8160, 1.8979]])\n",
      "[[1.816031  1.8979254]]\n",
      "1\n",
      "tensor([[1.8433, 1.9387]])\n",
      "tensor([[1.8433, 1.9387]])\n",
      "[[1.8432649 1.9386691]]\n",
      "1\n",
      "tensor([[1.7692, 1.8721]])\n",
      "tensor([[1.7692, 1.8721]])\n",
      "[[1.7692252 1.8720995]]\n",
      "1\n",
      "tensor([[1.5538, 1.6414]])\n",
      "tensor([[1.5538, 1.6414]])\n",
      "[[1.5538026 1.6413934]]\n",
      "1\n",
      "tensor([[1.3157, 1.3846]])\n",
      "tensor([[1.3157, 1.3846]])\n",
      "[[1.3157102 1.3845638]]\n",
      "1\n",
      "Episode 284\tAverage Score: 10.85tensor([[1.6894, 1.7340]])\n",
      "tensor([[1.6894, 1.7340]])\n",
      "[[1.6893775 1.734042 ]]\n",
      "1\n",
      "tensor([[1.6892, 1.7333]])\n",
      "tensor([[1.6892, 1.7333]])\n",
      "[[1.6891558 1.7333462]]\n",
      "1\n",
      "tensor([[1.7095, 1.7798]])\n",
      "tensor([[1.7095, 1.7798]])\n",
      "[[1.7094737 1.7798151]]\n",
      "1\n",
      "tensor([[1.7547, 1.8366]])\n",
      "tensor([[1.7547, 1.8366]])\n",
      "[[1.7546968 1.8365878]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DQagent = DQAgent(state_shape=env.observation_space.shape[0], action_space_size=env.action_space.n, seed=0)\n",
    "scores_dqn = dqn(DQagent)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training the `DQagent_Without_Target` by avoiding the Target network\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DQagent_Without_Target = DQAgent_Without_Target(state_shape=env.observation_space.shape[0], action_space_size=env.action_space.n, seed=0)\n",
    "scores_dqn_without_target = dqn(DQagent_Without_Target)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Comparing DQN and  DQN-Without_Target\n",
    "#### Plotting the time series of scores (scores_dqn & scores_dqn_without_target )\n",
    "\n",
    "I can use [Pandas](https://pandas.pydata.org/) to quickly plot the time series of scores along with a 100 episode moving average. Note that training stops as soon as the rolling average crosses the target score."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores_dqn   = pd.Series(scores_dqn , name=\"scores\")\n",
    "scores_dqn .describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores_dqn_without_target   = pd.Series(scores_dqn_without_target , name=\"scores\")\n",
    "scores_dqn_without_target .describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(10, 6), sharex=True, sharey=True)\n",
    "_ = scores_dqn.plot(ax=ax[0], label=\"Scores\")\n",
    "_ = (scores_dqn.rolling(window=100)\n",
    "           .mean()\n",
    "           .rename(\"Rolling Average\")\n",
    "           .plot(ax=ax[0]))\n",
    "_=ax[0].legend()\n",
    "_=ax[0].set_ylabel(\"DQN Score\")\n",
    "\n",
    "_ = scores_dqn_without_target.plot(ax=ax[1], label=\"Scores\")\n",
    "_ = (scores_dqn_without_target.rolling(window=100)\n",
    "           .mean()\n",
    "           .rename(\"Rolling Average\")\n",
    "           .plot(ax=ax[1]))\n",
    "_=ax[1].legend()\n",
    "_ = ax[1].set_ylabel(\"Score_without_target\")\n",
    "_ = ax[1].set_xlabel(\"Episode Number\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
