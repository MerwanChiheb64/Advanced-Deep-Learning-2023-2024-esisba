{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Deep Q-Network (DQN)\n",
    "---\n",
    "In this notebook, you will implement a DQN agent with OpenAI Gym's LunarLander-v2 environment.\n",
    "\n",
    "### Import the Necessary Packages"
   ],
   "metadata": {
    "collapsed": false,
    "id": "2p2cCsCnfP22"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install setuptools==65.5.0"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "id": "IQ0HwVwqoyR5",
    "outputId": "fafa7f4e-e94d-42b9-c81e-5507c2d78d98"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "!apt install python-opengl\n",
    "!apt install ffmpeg\n",
    "!apt install xvfb\n",
    "!apt install swig cmake\n",
    "!pip install pyglet==1.5\n",
    "!pip3 install pyvirtualdisplay"
   ],
   "metadata": {
    "id": "DRmbQzgPo133"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install gym==0.22\n",
    "!pip install imageio-ffmpeg\n",
    "!pip install gym[box2d]==0.22"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 896
    },
    "id": "WaLVidf0o6ug",
    "outputId": "05c8483b-d932-432e-a517-9375601626f6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ],
   "metadata": {
    "id": "y7AKn9TqfP25"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define some hyperparameter"
   ],
   "metadata": {
    "collapsed": false,
    "id": "d7Zq_7HAfP27"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate\n",
    "UPDATE_EVERY = 4        # how often to update the network"
   ],
   "metadata": {
    "id": "XmdvUFEDfP27"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "id": "16_GML3nfP27"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define Neural Network Architecture.\n",
    "\n",
    "Since `LunarLander-v2` environment is sort of simple envs, we don't need complicated architecture. We just need non-linear function approximator that maps from state to action."
   ],
   "metadata": {
    "collapsed": false,
    "id": "9J83ze82fP27"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, state_shape, action_space_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_shape (int): Dimension of each state\n",
    "            action_space_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        ## add three fully connected layers using nn.Linear()\n",
    "        ##fc1(input_shape,64)   fc2(64,64)  fc2(64,output)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        # define the forward propagation  by adding the relu() activation function between layers\n",
    "        return x"
   ],
   "metadata": {
    "id": "6YNclskqfP28"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define Replay Buffer\n",
    "### Experience Replay\n",
    "\n",
    "To perform *experience replay* the authors store the agent's experiences $e_t$ as represented by the tuple\n",
    "\n",
    "$$ e_t = (s_t, a_t, r_t, s_{t+1}) $$\n",
    "\n",
    "consisting of the observed state in period $t$, the reward received in period $t$, the action taken in period $t$, and the resulting state in period $t+1$. The dataset of agent experiences at period $t$ consists of the set of past experiences.\n",
    "\n",
    "$$ D_t = \\{e1, e2, ..., e_t \\} $$\n",
    "\n",
    "Depending on the task it may note be feasible for the agent to store the entire history of past experiences.\n",
    "\n",
    "During learning Q-learning updates are computed based on samples (or minibatches) of experience $(s,a,r,s')$, drawn uniformly at random from the pool of stored samples $D_t$.\n",
    "\n",
    "The following is my Python implmentation of these ideas.\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "OjcPyfQ7fP29"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ],
   "metadata": {
    "id": "hlIePvuHfP29"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define the Deep QLearning Agent\n",
    "\n",
    "The Deep $Q$-learning update at iteration $i$ uses the following loss function\n",
    "\n",
    "$$ \\mathcal{L_i}(\\theta_i) = \\mathbb{E}_{(s, a, r, s') \\sim U(D)} \\Bigg[\\bigg(r + \\gamma \\max_{a'} Q\\big(s', a'; \\theta_i^{-}\\big) - Q\\big(s, a; \\theta_i\\big)\\bigg)^2\\Bigg] $$\n",
    "\n",
    "where $\\gamma$ is the discount factor determining the agentâ€™s horizon, $\\theta_i$ are the parameters of the $Q$-network at iteration $i$ and $\\theta_i^{-}$ are the $Q$-network parameters used to compute the target at iteration $i$. The target network parameters $\\theta_i^{-}$ are only updated with the $Q$-network parameters $\\theta_i$ every $C$ steps and are held fixed between individual updates.\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "5ESIilDXfP2-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DQAgent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_shape, action_space_size, seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state_shape (int): dimension of each state\n",
    "            action_space_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_shape = state_shape\n",
    "        self.action_space_size = action_space_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        # Q-Network\n",
    "        # create the local and the target networks\n",
    "        self.qnetwork_local = #--add code here---\n",
    "        self.qnetwork_target = #--add code here---\n",
    "\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = #--add code here---\n",
    "\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "\n",
    "\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            #convert the state array to a tensor by adding a dimension\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "            #predict the QValue for each possible state\n",
    "            action_Qvalues =#--add code here---\n",
    "\n",
    "            #return the action with the highest Qvalue\n",
    "            return np.argmax(#--add code here---)\n",
    "        else:\n",
    "            # return a random action\n",
    "            return random.choice(np.arange(self.action_space_size))\n",
    "\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        # Obtain random minibatch of tuples from D\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        ## Compute and minimize the loss\n",
    "        ### Extract next maximum estimated value from target network\n",
    "        q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "\n",
    "        ### Calculate target value from bellman equation\n",
    "        q_targets = #--add code here---\n",
    "\n",
    "        ### Calculate expected value from local network\n",
    "        q_expected = #--add code here---\n",
    "\n",
    "        ### Loss calculation (we used Mean squared error)\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        Î¸_target = Ï„*Î¸_local + (1 - Ï„)*Î¸_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        #--add code here---\n",
    "\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > #--add code here---:\n",
    "                # get some experiences from the memory\n",
    "                experiences = #--add code here---\n",
    "\n",
    "                # train the local Q-network\n",
    "                #--add code here---"
   ],
   "metadata": {
    "id": "yy-v66AgfP2_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "id": "XQAQGS7ofP3A"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training Process\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "tCyWci_YfP3A"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def dqn(n_episodes=1000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            #get an action\n",
    "            action = #--add code here---\n",
    "\n",
    "            #execute the action on the environment\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # store the current experience and launch the training of the local Q-Network\n",
    "            #--add code here---\n",
    "            state =  #--add code here---\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "\n",
    "    torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "    return scores\n"
   ],
   "metadata": {
    "id": "aaTZRN5TfP3A"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LunarLander-v2\n",
    "<img src=\"https://www.gymlibrary.dev/_images/lunar_lander.gif\" alt=\"LunarLander-v2\"  width=\"100%\"/>\n",
    "\n",
    "This environment is part of the Box2D environments. Please read that page first for general information.\n",
    "https://www.gymlibrary.dev/environments/box2d/lunar_lander/\n",
    "\n",
    "### Action Space\n",
    "There are four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine.\n",
    "\n",
    "### Observation Space\n",
    "The state is an 8-dimensional vector: the coordinates of the lander in x & y, its linear velocities in x & y, its angle, its angular velocity, and two booleans that represent whether each leg is in contact with the ground or not.\n",
    "\n",
    "### Rewards\n",
    "Reward for moving from the top of the screen to the landing pad and coming to rest is about 100-140 points. If the lander moves away from the landing pad, it loses reward. If the lander crashes, it receives an additional -100 points. If it comes to rest, it receives an additional +100 points. Each leg with ground contact is +10 points. Firing the main engine is -0.3 points each frame. Firing the side engine is -0.03 points each frame. Solved is 200"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Lq93jPK0fP3B"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "print(env.reset())\n",
    "print('State shape: ', env.observation_space.shape[0])\n",
    "print('Number of actions: ', env.action_space.n)\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Amss4kNwfP3B",
    "outputId": "0da4b16e-a5ae-4a82-f2dc-8b4d5a0bdcd3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "id": "Sh5Gxq7DfP3B"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent = DQAgent(state_shape=env.observation_space.shape[0], action_space_size=env.action_space.n, seed=0)\n",
    "scores = dqn()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qUYrNolWfP3B",
    "outputId": "ee4e271c-7e3c-48de-ba7f-101c145db2c0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "id": "H_byovfYfP3C"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Plotting the time series of scores\n",
    "\n",
    "I can use [Pandas](https://pandas.pydata.org/) to quickly plot the time series of scores along with a 100 episode moving average. Note that training stops as soon as the rolling average crosses the target score."
   ],
   "metadata": {
    "collapsed": false,
    "id": "m77NKqKMfP3C"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores = pd.Series(scores, name=\"scores\")\n",
    "scores.describe()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TEck5yEZfP3C",
    "outputId": "ba51c957-93e3-466b-f80a-367ca9cc7b3c"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "_ = scores.plot(ax=ax, label=\"Scores\")\n",
    "_ = (scores.rolling(window=100)\n",
    "           .mean()\n",
    "           .rename(\"Rolling Average\")\n",
    "           .plot(ax=ax))\n",
    "ax.legend()\n",
    "_ = ax.set_xlabel(\"Episode Number\")\n",
    "_ = ax.set_ylabel(\"Score\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "a5tFWRVrfP3D",
    "outputId": "28cd22de-d1da-4fd5-fd3c-6705a920d8f8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Performance of an un-trained `DeepQAgent`\n",
    "\n",
    "The function `simulate` defined in the cell below can be used to simuate an agent interacting with and environment for one episode."
   ],
   "metadata": {
    "collapsed": false,
    "id": "XE5CvR8XfP3D"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    os.environ[\"DISPLAY\"]\n",
    "except:\n",
    "    os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "\n",
    "def record_video(env, agent, out_directory, fps=1):\n",
    "  \"\"\"\n",
    "  Generate a replay video of the agent\n",
    "  :param env\n",
    "  :param agent:  agent within its Qtable\n",
    "  :param out_directory\n",
    "  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n",
    "  \"\"\"\n",
    "  images = []\n",
    "  done = False\n",
    "  state= env.reset()\n",
    "  img = env.render(mode='rgb_array')\n",
    "  images.append(img)\n",
    "  while not done:\n",
    "    # Take the action (index) that have the maximum expected future reward given that state\n",
    "    action = agent.act(state)\n",
    "    state, reward, done, _ = env.step(action) # We directly put next_state = state for recording logic\n",
    "    img = env.render(mode='rgb_array')\n",
    "    images.append(img)\n",
    "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
   ],
   "metadata": {
    "id": "qbYpgKAqfP3E"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "id": "1XkkeQ_HfP3E"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from base64 import b64encode\n",
    "from IPython.display import HTML\n",
    "\n",
    "# generate the video\n",
    "video_path = \"./replay.mp4\"\n",
    "record_video(env, agent, video_path, 1)\n",
    "\n",
    "# Show video\n",
    "mp4 = open(video_path,'rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\"\"\"<video width=400 controls>      <source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % data_url)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "Ri80QEnSfP3E",
    "outputId": "4b83ff7f-85f9-443d-e28d-53350b6be481"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "DHOcR3em1fZX"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
